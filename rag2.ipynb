{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishna0rothe/RAG_Models/blob/main/rag2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBmvT2rowO5f"
      },
      "outputs": [],
      "source": [
        "%%writefile requirements.txt\n",
        "langchain\n",
        "langchain-community\n",
        "llama-parse\n",
        "fastembed\n",
        "chromadb\n",
        "python-dotenv\n",
        "langchain-groq\n",
        "chainlit\n",
        "fastembed\n",
        "unstructured[md]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-g1EK1pwU23"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3M2sQuHwkAV"
      },
      "outputs": [],
      "source": [
        "# Add you api key's here\n",
        "llamaparse_api_key = \"\"\n",
        "groq_api_key = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOc4Q00-woCG"
      },
      "outputs": [],
      "source": [
        "##### LLAMAPARSE #####\n",
        "from llama_parse import LlamaParse\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "#\n",
        "from groq import Groq\n",
        "from langchain_groq import ChatGroq\n",
        "#\n",
        "import joblib\n",
        "import os\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtBrXFNKwtJy"
      },
      "outputs": [],
      "source": [
        "\n",
        "def load_or_parse_data():\n",
        "    data_file = \"./data/parsed_data3.pkl\"\n",
        "\n",
        "    if os.path.exists(data_file):\n",
        "        # Load the parsed data from the file\n",
        "        parsed_data = joblib.load(data_file)\n",
        "    else:\n",
        "        # Perform the parsing step and store the result in llama_parse_documents\n",
        "        parsingInstruction = \"\"\"\n",
        "        Try to be precise while answering the questions\"\"\"\n",
        "        parser = LlamaParse(api_key=llamaparse_api_key,\n",
        "                            result_type=\"markdown\",\n",
        "                            parsing_instruction=parsingInstruction,\n",
        "                            max_timeout=5000,)\n",
        "        llama_parse_documents = parser.load_data(\"/TECH TRIAD WORKSHOP Report.pdf\")\n",
        "\n",
        "\n",
        "        # Save the parsed data to a file\n",
        "        print(\"Saving the parse results in .pkl format ..........\")\n",
        "        joblib.dump(llama_parse_documents, data_file)\n",
        "\n",
        "        # Set the parsed data to the variable\n",
        "        parsed_data = llama_parse_documents\n",
        "\n",
        "    return parsed_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDKFY_5nwyZ4"
      },
      "outputs": [],
      "source": [
        "# Create vector database\n",
        "def create_vector_database():\n",
        "    \"\"\"\n",
        "    Creates a vector database using document loaders and embeddings.\n",
        "\n",
        "    This function loads urls,\n",
        "    splits the loaded documents into chunks, transforms them into embeddings using OllamaEmbeddings,\n",
        "    and finally persists the embeddings into a Chroma vector database.\n",
        "\n",
        "    \"\"\"\n",
        "    # Call the function to either load or parse the data\n",
        "    llama_parse_documents = load_or_parse_data()\n",
        "    print(llama_parse_documents[0].text[:300])\n",
        "\n",
        "    with open('data/output3.md', 'a') as f:\n",
        "        for doc in llama_parse_documents:\n",
        "            f.write(doc.text + '\\n')\n",
        "\n",
        "    markdown_path = \"/content/data/output3.md\"\n",
        "    loader = UnstructuredMarkdownLoader(markdown_path)\n",
        "\n",
        "   #loader = DirectoryLoader('data/', glob=\"**/*.md\", show_progress=True)\n",
        "    documents = loader.load()\n",
        "    # Split loaded documents into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=100)\n",
        "    docs = text_splitter.split_documents(documents)\n",
        "\n",
        "    #len(docs)\n",
        "    print(f\"length of documents loaded: {len(documents)}\")\n",
        "    print(f\"total number of document chunks generated :{len(docs)}\")\n",
        "    #docs[0]\n",
        "\n",
        "    # Initialize Embeddings\n",
        "    embed_model = FastEmbedEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n",
        "\n",
        "    # Create and persist a Chroma vector database from the chunked documents\n",
        "    vs = Chroma.from_documents(\n",
        "        documents=docs,\n",
        "        embedding=embed_model,\n",
        "        persist_directory=\"chroma_db_llamaparse1\",\n",
        "        collection_name=\"rag\"\n",
        "    )\n",
        "\n",
        "\n",
        "    print('Vector DB created successfully !')\n",
        "    return vs,embed_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtRgMBbxw4K6"
      },
      "outputs": [],
      "source": [
        "vs,embed_model = create_vector_database()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIGK4Meiw65Z"
      },
      "outputs": [],
      "source": [
        "chat_model = ChatGroq(temperature=0,\n",
        "                      model_name=\"mixtral-8x7b-32768\",\n",
        "                      api_key= groq_api_key )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gM72LAOMxOum"
      },
      "outputs": [],
      "source": [
        "vectorstore = Chroma(embedding_function=embed_model,\n",
        "                      persist_directory=\"chroma_db_llamaparse1\",\n",
        "                      collection_name=\"rag\")\n",
        " #\n",
        "retriever=vectorstore.as_retriever(search_kwargs={'k': 3})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EB7v0aJUxUGP"
      },
      "outputs": [],
      "source": [
        "custom_prompt_template = \"\"\"Use the following pieces of information to answer the user's question.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "\n",
        "Only return the helpful answer below and nothing else.\n",
        "Helpful answer:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PLTq0AwxZgK"
      },
      "outputs": [],
      "source": [
        "def set_custom_prompt():\n",
        "\n",
        "    prompt = PromptTemplate(template=custom_prompt_template,\n",
        "                            input_variables=['context', 'question'])\n",
        "    return prompt\n",
        "#\n",
        "prompt = set_custom_prompt()\n",
        "prompt\n",
        "\n",
        "########################### RESPONSE ###########################\n",
        "PromptTemplate(input_variables=['context', 'question'], template=\"Use the following pieces of information to answer the user's question.\\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nContext: {context}\\nQuestion: {question}\\n\\nOnly return the helpful answer below and nothing else.\\nHelpful answer:\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmKzrzMExcmk"
      },
      "outputs": [],
      "source": [
        "qa = RetrievalQA.from_chain_type(llm=chat_model,\n",
        "                               chain_type=\"stuff\",\n",
        "                               retriever=retriever,\n",
        "                               return_source_documents=True,\n",
        "                               chain_type_kwargs={\"prompt\": prompt})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZxz-rIQyLM-"
      },
      "outputs": [],
      "source": [
        "query =\"which topic got lowest percentage?\"\n",
        "response = qa.invoke({\"query\":query})\n",
        "response['result']"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMS1ygGZneUSlRaZI4J8NOj",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}